{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import layers,callbacks,utils,applications,optimizers\n",
    "from keras.models import Sequential,Model,load_model\n",
    "from keras.utils import Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "558/558 [==============================] - 184s 241ms/step - loss: 1.8658 - mae: 1.8658 - val_loss: 2.1096 - val_mae: 2.1096 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "558/558 [==============================] - 131s 236ms/step - loss: 0.7470 - mae: 0.7470 - val_loss: 0.6475 - val_mae: 0.6475 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "558/558 [==============================] - 135s 241ms/step - loss: 0.5917 - mae: 0.5917 - val_loss: 0.3869 - val_mae: 0.3869 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "558/558 [==============================] - 130s 233ms/step - loss: 0.5348 - mae: 0.5348 - val_loss: 0.4968 - val_mae: 0.4968 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "558/558 [==============================] - 135s 242ms/step - loss: 0.5000 - mae: 0.5000 - val_loss: 0.6184 - val_mae: 0.6184 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "558/558 [==============================] - 134s 240ms/step - loss: 0.4864 - mae: 0.4864 - val_loss: 0.2293 - val_mae: 0.2293 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "558/558 [==============================] - 132s 237ms/step - loss: 0.4547 - mae: 0.4547 - val_loss: 0.3192 - val_mae: 0.3192 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "558/558 [==============================] - 2331s 4s/step - loss: 0.4652 - mae: 0.4652 - val_loss: 0.9858 - val_mae: 0.9858 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "558/558 [==============================] - 128s 229ms/step - loss: 0.4576 - mae: 0.4576 - val_loss: 0.7444 - val_mae: 0.7444 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "558/558 [==============================] - 130s 233ms/step - loss: 0.4235 - mae: 0.4235 - val_loss: 0.3414 - val_mae: 0.3414 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.4130 - mae: 0.4130\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
      "558/558 [==============================] - 131s 234ms/step - loss: 0.4130 - mae: 0.4130 - val_loss: 0.7326 - val_mae: 0.7326 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "558/558 [==============================] - 131s 236ms/step - loss: 0.3967 - mae: 0.3967 - val_loss: 0.1646 - val_mae: 0.1646 - lr: 9.0000e-04\n",
      "Epoch 13/100\n",
      "558/558 [==============================] - 131s 235ms/step - loss: 0.4007 - mae: 0.4007 - val_loss: 0.1172 - val_mae: 0.1172 - lr: 9.0000e-04\n",
      "Epoch 14/100\n",
      "558/558 [==============================] - 130s 234ms/step - loss: 0.3818 - mae: 0.3818 - val_loss: 0.2154 - val_mae: 0.2154 - lr: 9.0000e-04\n",
      "Epoch 15/100\n",
      "558/558 [==============================] - 130s 234ms/step - loss: 0.4176 - mae: 0.4176 - val_loss: 0.2508 - val_mae: 0.2508 - lr: 9.0000e-04\n",
      "Epoch 16/100\n",
      "558/558 [==============================] - 131s 234ms/step - loss: 0.3825 - mae: 0.3825 - val_loss: 0.2225 - val_mae: 0.2225 - lr: 9.0000e-04\n",
      "Epoch 17/100\n",
      "558/558 [==============================] - 152s 273ms/step - loss: 0.3784 - mae: 0.3784 - val_loss: 0.6465 - val_mae: 0.6465 - lr: 9.0000e-04\n",
      "Epoch 18/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.3867 - mae: 0.3867\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
      "558/558 [==============================] - 157s 281ms/step - loss: 0.3867 - mae: 0.3867 - val_loss: 0.2175 - val_mae: 0.2175 - lr: 9.0000e-04\n",
      "Epoch 19/100\n",
      "558/558 [==============================] - 158s 283ms/step - loss: 0.3554 - mae: 0.3554 - val_loss: 0.1973 - val_mae: 0.1973 - lr: 8.1000e-04\n",
      "Epoch 20/100\n",
      "558/558 [==============================] - 167s 299ms/step - loss: 0.3516 - mae: 0.3516 - val_loss: 0.1504 - val_mae: 0.1504 - lr: 8.1000e-04\n",
      "Epoch 21/100\n",
      "558/558 [==============================] - 164s 294ms/step - loss: 0.3620 - mae: 0.3620 - val_loss: 0.4628 - val_mae: 0.4628 - lr: 8.1000e-04\n",
      "Epoch 22/100\n",
      "558/558 [==============================] - 163s 292ms/step - loss: 0.3508 - mae: 0.3508 - val_loss: 0.1884 - val_mae: 0.1884 - lr: 8.1000e-04\n",
      "Epoch 23/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.3531 - mae: 0.3531\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
      "558/558 [==============================] - 167s 299ms/step - loss: 0.3531 - mae: 0.3531 - val_loss: 0.3846 - val_mae: 0.3846 - lr: 8.1000e-04\n",
      "Epoch 24/100\n",
      "558/558 [==============================] - 166s 297ms/step - loss: 0.3376 - mae: 0.3376 - val_loss: 0.1148 - val_mae: 0.1148 - lr: 7.2900e-04\n",
      "Epoch 25/100\n",
      "558/558 [==============================] - 160s 286ms/step - loss: 0.3461 - mae: 0.3461 - val_loss: 0.4358 - val_mae: 0.4358 - lr: 7.2900e-04\n",
      "Epoch 26/100\n",
      "558/558 [==============================] - 161s 289ms/step - loss: 0.3407 - mae: 0.3407 - val_loss: 0.1221 - val_mae: 0.1221 - lr: 7.2900e-04\n",
      "Epoch 27/100\n",
      "558/558 [==============================] - 161s 288ms/step - loss: 0.3364 - mae: 0.3364 - val_loss: 0.1389 - val_mae: 0.1389 - lr: 7.2900e-04\n",
      "Epoch 28/100\n",
      "558/558 [==============================] - 161s 289ms/step - loss: 0.3374 - mae: 0.3374 - val_loss: 0.4819 - val_mae: 0.4819 - lr: 7.2900e-04\n",
      "Epoch 29/100\n",
      "558/558 [==============================] - 162s 291ms/step - loss: 0.3366 - mae: 0.3366 - val_loss: 0.0992 - val_mae: 0.0992 - lr: 7.2900e-04\n",
      "Epoch 30/100\n",
      "558/558 [==============================] - 1957s 4s/step - loss: 0.3289 - mae: 0.3289 - val_loss: 0.0886 - val_mae: 0.0886 - lr: 7.2900e-04\n",
      "Epoch 31/100\n",
      "558/558 [==============================] - 163s 291ms/step - loss: 0.3389 - mae: 0.3389 - val_loss: 0.1733 - val_mae: 0.1733 - lr: 7.2900e-04\n",
      "Epoch 32/100\n",
      "558/558 [==============================] - 163s 292ms/step - loss: 0.3343 - mae: 0.3343 - val_loss: 0.1329 - val_mae: 0.1329 - lr: 7.2900e-04\n",
      "Epoch 33/100\n",
      "558/558 [==============================] - 148s 265ms/step - loss: 0.3292 - mae: 0.3292 - val_loss: 0.2084 - val_mae: 0.2084 - lr: 7.2900e-04\n",
      "Epoch 34/100\n",
      "558/558 [==============================] - 132s 236ms/step - loss: 0.3305 - mae: 0.3305 - val_loss: 0.2474 - val_mae: 0.2474 - lr: 7.2900e-04\n",
      "Epoch 35/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.3251 - mae: 0.3251\n",
      "Epoch 35: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
      "558/558 [==============================] - 131s 235ms/step - loss: 0.3251 - mae: 0.3251 - val_loss: 0.1102 - val_mae: 0.1102 - lr: 7.2900e-04\n",
      "Epoch 36/100\n",
      "558/558 [==============================] - 131s 234ms/step - loss: 0.3177 - mae: 0.3177 - val_loss: 0.1610 - val_mae: 0.1610 - lr: 6.5610e-04\n",
      "Epoch 37/100\n",
      "558/558 [==============================] - 130s 234ms/step - loss: 0.3137 - mae: 0.3137 - val_loss: 0.0946 - val_mae: 0.0946 - lr: 6.5610e-04\n",
      "Epoch 38/100\n",
      "558/558 [==============================] - 130s 234ms/step - loss: 0.3129 - mae: 0.3129 - val_loss: 0.2679 - val_mae: 0.2679 - lr: 6.5610e-04\n",
      "Epoch 39/100\n",
      "558/558 [==============================] - 130s 233ms/step - loss: 0.3084 - mae: 0.3084 - val_loss: 0.2181 - val_mae: 0.2181 - lr: 6.5610e-04\n",
      "Epoch 40/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.3190 - mae: 0.3190\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
      "558/558 [==============================] - 130s 233ms/step - loss: 0.3190 - mae: 0.3190 - val_loss: 0.1369 - val_mae: 0.1369 - lr: 6.5610e-04\n",
      "Epoch 41/100\n",
      "558/558 [==============================] - 130s 233ms/step - loss: 0.3074 - mae: 0.3074 - val_loss: 0.3190 - val_mae: 0.3190 - lr: 5.9049e-04\n",
      "Epoch 42/100\n",
      "558/558 [==============================] - 131s 235ms/step - loss: 0.3056 - mae: 0.3056 - val_loss: 0.0716 - val_mae: 0.0716 - lr: 5.9049e-04\n",
      "Epoch 43/100\n",
      "558/558 [==============================] - 130s 233ms/step - loss: 0.3033 - mae: 0.3033 - val_loss: 0.0782 - val_mae: 0.0782 - lr: 5.9049e-04\n",
      "Epoch 44/100\n",
      "558/558 [==============================] - 136s 244ms/step - loss: 0.3056 - mae: 0.3056 - val_loss: 0.0967 - val_mae: 0.0967 - lr: 5.9049e-04\n",
      "Epoch 45/100\n",
      "558/558 [==============================] - 130s 233ms/step - loss: 0.3024 - mae: 0.3024 - val_loss: 0.1696 - val_mae: 0.1696 - lr: 5.9049e-04\n",
      "Epoch 46/100\n",
      "558/558 [==============================] - 130s 233ms/step - loss: 0.3054 - mae: 0.3054 - val_loss: 0.1798 - val_mae: 0.1798 - lr: 5.9049e-04\n",
      "Epoch 47/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.3001 - mae: 0.3001\n",
      "Epoch 47: ReduceLROnPlateau reducing learning rate to 0.0005314410547725857.\n",
      "558/558 [==============================] - 130s 233ms/step - loss: 0.3001 - mae: 0.3001 - val_loss: 0.2190 - val_mae: 0.2190 - lr: 5.9049e-04\n",
      "Epoch 48/100\n",
      "558/558 [==============================] - 130s 233ms/step - loss: 0.2972 - mae: 0.2972 - val_loss: 0.1107 - val_mae: 0.1107 - lr: 5.3144e-04\n",
      "Epoch 49/100\n",
      "558/558 [==============================] - 130s 232ms/step - loss: 0.2971 - mae: 0.2971 - val_loss: 0.0959 - val_mae: 0.0959 - lr: 5.3144e-04\n",
      "Epoch 50/100\n",
      "558/558 [==============================] - 130s 232ms/step - loss: 0.2973 - mae: 0.2973 - val_loss: 0.0943 - val_mae: 0.0943 - lr: 5.3144e-04\n",
      "Epoch 51/100\n",
      "558/558 [==============================] - 4608s 8s/step - loss: 0.3013 - mae: 0.3013 - val_loss: 0.1346 - val_mae: 0.1346 - lr: 5.3144e-04\n",
      "Epoch 52/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.2878 - mae: 0.2878\n",
      "Epoch 52: ReduceLROnPlateau reducing learning rate to 0.00047829695977270604.\n",
      "558/558 [==============================] - 120s 214ms/step - loss: 0.2878 - mae: 0.2878 - val_loss: 0.1102 - val_mae: 0.1102 - lr: 5.3144e-04\n",
      "Epoch 53/100\n",
      "558/558 [==============================] - 130s 232ms/step - loss: 0.2913 - mae: 0.2913 - val_loss: 0.0627 - val_mae: 0.0627 - lr: 4.7830e-04\n",
      "Epoch 54/100\n",
      "558/558 [==============================] - 129s 231ms/step - loss: 0.2879 - mae: 0.2879 - val_loss: 0.0804 - val_mae: 0.0804 - lr: 4.7830e-04\n",
      "Epoch 55/100\n",
      "558/558 [==============================] - 129s 232ms/step - loss: 0.2868 - mae: 0.2868 - val_loss: 0.0880 - val_mae: 0.0880 - lr: 4.7830e-04\n",
      "Epoch 56/100\n",
      "558/558 [==============================] - 130s 232ms/step - loss: 0.2825 - mae: 0.2825 - val_loss: 0.0975 - val_mae: 0.0975 - lr: 4.7830e-04\n",
      "Epoch 57/100\n",
      "558/558 [==============================] - 130s 232ms/step - loss: 0.2851 - mae: 0.2851 - val_loss: 0.3654 - val_mae: 0.3654 - lr: 4.7830e-04\n",
      "Epoch 58/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.2892 - mae: 0.2892\n",
      "Epoch 58: ReduceLROnPlateau reducing learning rate to 0.0004304672533180565.\n",
      "558/558 [==============================] - 130s 233ms/step - loss: 0.2892 - mae: 0.2892 - val_loss: 0.0768 - val_mae: 0.0768 - lr: 4.7830e-04\n",
      "Epoch 59/100\n",
      "558/558 [==============================] - 2615s 5s/step - loss: 0.2831 - mae: 0.2831 - val_loss: 0.0980 - val_mae: 0.0980 - lr: 4.3047e-04\n",
      "Epoch 60/100\n",
      "558/558 [==============================] - 120s 215ms/step - loss: 0.2777 - mae: 0.2777 - val_loss: 0.1668 - val_mae: 0.1668 - lr: 4.3047e-04\n",
      "Epoch 61/100\n",
      "558/558 [==============================] - 129s 231ms/step - loss: 0.2804 - mae: 0.2804 - val_loss: 0.0968 - val_mae: 0.0968 - lr: 4.3047e-04\n",
      "Epoch 62/100\n",
      "558/558 [==============================] - 129s 231ms/step - loss: 0.2822 - mae: 0.2822 - val_loss: 0.0905 - val_mae: 0.0905 - lr: 4.3047e-04\n",
      "Epoch 63/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.2769 - mae: 0.2769\n",
      "Epoch 63: ReduceLROnPlateau reducing learning rate to 0.00038742052274756136.\n",
      "558/558 [==============================] - 130s 233ms/step - loss: 0.2769 - mae: 0.2769 - val_loss: 0.0773 - val_mae: 0.0773 - lr: 4.3047e-04\n",
      "Epoch 64/100\n",
      "558/558 [==============================] - 129s 231ms/step - loss: 0.2759 - mae: 0.2759 - val_loss: 0.0840 - val_mae: 0.0840 - lr: 3.8742e-04\n",
      "Epoch 65/100\n",
      "558/558 [==============================] - 129s 232ms/step - loss: 0.2756 - mae: 0.2756 - val_loss: 0.0859 - val_mae: 0.0859 - lr: 3.8742e-04\n",
      "Epoch 66/100\n",
      "558/558 [==============================] - 129s 232ms/step - loss: 0.2766 - mae: 0.2766 - val_loss: 0.0676 - val_mae: 0.0676 - lr: 3.8742e-04\n",
      "Epoch 67/100\n",
      "558/558 [==============================] - 131s 234ms/step - loss: 0.2761 - mae: 0.2761 - val_loss: 0.0443 - val_mae: 0.0443 - lr: 3.8742e-04\n",
      "Epoch 68/100\n",
      "558/558 [==============================] - 132s 236ms/step - loss: 0.2739 - mae: 0.2739 - val_loss: 0.0720 - val_mae: 0.0720 - lr: 3.8742e-04\n",
      "Epoch 69/100\n",
      "558/558 [==============================] - 131s 235ms/step - loss: 0.2715 - mae: 0.2715 - val_loss: 0.0633 - val_mae: 0.0633 - lr: 3.8742e-04\n",
      "Epoch 70/100\n",
      "558/558 [==============================] - 130s 234ms/step - loss: 0.2718 - mae: 0.2718 - val_loss: 0.0992 - val_mae: 0.0992 - lr: 3.8742e-04\n",
      "Epoch 71/100\n",
      "558/558 [==============================] - 131s 235ms/step - loss: 0.2750 - mae: 0.2750 - val_loss: 0.0925 - val_mae: 0.0925 - lr: 3.8742e-04\n",
      "Epoch 72/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.2718 - mae: 0.2718\n",
      "Epoch 72: ReduceLROnPlateau reducing learning rate to 0.0003486784757114947.\n",
      "558/558 [==============================] - 133s 239ms/step - loss: 0.2718 - mae: 0.2718 - val_loss: 0.1323 - val_mae: 0.1323 - lr: 3.8742e-04\n",
      "Epoch 73/100\n",
      "558/558 [==============================] - 132s 236ms/step - loss: 0.2706 - mae: 0.2706 - val_loss: 0.2536 - val_mae: 0.2536 - lr: 3.4868e-04\n",
      "Epoch 74/100\n",
      "558/558 [==============================] - 132s 236ms/step - loss: 0.2644 - mae: 0.2644 - val_loss: 0.0883 - val_mae: 0.0883 - lr: 3.4868e-04\n",
      "Epoch 75/100\n",
      "558/558 [==============================] - 132s 237ms/step - loss: 0.2681 - mae: 0.2681 - val_loss: 0.1172 - val_mae: 0.1172 - lr: 3.4868e-04\n",
      "Epoch 76/100\n",
      "558/558 [==============================] - 132s 237ms/step - loss: 0.2675 - mae: 0.2675 - val_loss: 0.1833 - val_mae: 0.1833 - lr: 3.4868e-04\n",
      "Epoch 77/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.2694 - mae: 0.2694\n",
      "Epoch 77: ReduceLROnPlateau reducing learning rate to 0.00031381062290165574.\n",
      "558/558 [==============================] - 132s 236ms/step - loss: 0.2694 - mae: 0.2694 - val_loss: 0.1345 - val_mae: 0.1345 - lr: 3.4868e-04\n",
      "Epoch 78/100\n",
      "558/558 [==============================] - 132s 237ms/step - loss: 0.2664 - mae: 0.2664 - val_loss: 0.0577 - val_mae: 0.0577 - lr: 3.1381e-04\n",
      "Epoch 79/100\n",
      "558/558 [==============================] - 132s 237ms/step - loss: 0.2639 - mae: 0.2639 - val_loss: 0.0697 - val_mae: 0.0697 - lr: 3.1381e-04\n",
      "Epoch 80/100\n",
      "558/558 [==============================] - 134s 241ms/step - loss: 0.2639 - mae: 0.2639 - val_loss: 0.0522 - val_mae: 0.0522 - lr: 3.1381e-04\n",
      "Epoch 81/100\n",
      "558/558 [==============================] - 133s 238ms/step - loss: 0.2633 - mae: 0.2633 - val_loss: 0.0768 - val_mae: 0.0768 - lr: 3.1381e-04\n",
      "Epoch 82/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.2642 - mae: 0.2642\n",
      "Epoch 82: ReduceLROnPlateau reducing learning rate to 0.0002824295632308349.\n",
      "558/558 [==============================] - 131s 235ms/step - loss: 0.2642 - mae: 0.2642 - val_loss: 0.0616 - val_mae: 0.0616 - lr: 3.1381e-04\n",
      "Epoch 83/100\n",
      "558/558 [==============================] - 131s 236ms/step - loss: 0.2620 - mae: 0.2620 - val_loss: 0.1575 - val_mae: 0.1575 - lr: 2.8243e-04\n",
      "Epoch 84/100\n",
      "558/558 [==============================] - 131s 234ms/step - loss: 0.2658 - mae: 0.2658 - val_loss: 0.0672 - val_mae: 0.0672 - lr: 2.8243e-04\n",
      "Epoch 85/100\n",
      "558/558 [==============================] - 131s 235ms/step - loss: 0.2580 - mae: 0.2580 - val_loss: 0.0674 - val_mae: 0.0674 - lr: 2.8243e-04\n",
      "Epoch 86/100\n",
      "558/558 [==============================] - 131s 234ms/step - loss: 0.2620 - mae: 0.2620 - val_loss: 0.1140 - val_mae: 0.1140 - lr: 2.8243e-04\n",
      "Epoch 87/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.2594 - mae: 0.2594\n",
      "Epoch 87: ReduceLROnPlateau reducing learning rate to 0.00025418660952709616.\n",
      "558/558 [==============================] - 131s 235ms/step - loss: 0.2594 - mae: 0.2594 - val_loss: 0.0819 - val_mae: 0.0819 - lr: 2.8243e-04\n",
      "Epoch 88/100\n",
      "558/558 [==============================] - 131s 234ms/step - loss: 0.2600 - mae: 0.2600 - val_loss: 0.0849 - val_mae: 0.0849 - lr: 2.5419e-04\n",
      "Epoch 89/100\n",
      "558/558 [==============================] - 130s 234ms/step - loss: 0.2545 - mae: 0.2545 - val_loss: 0.0483 - val_mae: 0.0483 - lr: 2.5419e-04\n",
      "Epoch 90/100\n",
      "558/558 [==============================] - 136s 243ms/step - loss: 0.2552 - mae: 0.2552 - val_loss: 0.0618 - val_mae: 0.0618 - lr: 2.5419e-04\n",
      "Epoch 91/100\n",
      "558/558 [==============================] - 135s 242ms/step - loss: 0.2494 - mae: 0.2494 - val_loss: 0.0532 - val_mae: 0.0532 - lr: 2.5419e-04\n",
      "Epoch 92/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.2580 - mae: 0.2580\n",
      "Epoch 92: ReduceLROnPlateau reducing learning rate to 0.00022876793809700757.\n",
      "558/558 [==============================] - 135s 242ms/step - loss: 0.2580 - mae: 0.2580 - val_loss: 0.0604 - val_mae: 0.0604 - lr: 2.5419e-04\n",
      "Epoch 93/100\n",
      "558/558 [==============================] - 134s 241ms/step - loss: 0.2534 - mae: 0.2534 - val_loss: 0.1038 - val_mae: 0.1038 - lr: 2.2877e-04\n",
      "Epoch 94/100\n",
      "558/558 [==============================] - 134s 240ms/step - loss: 0.2532 - mae: 0.2532 - val_loss: 0.0498 - val_mae: 0.0498 - lr: 2.2877e-04\n",
      "Epoch 95/100\n",
      "558/558 [==============================] - 134s 240ms/step - loss: 0.2541 - mae: 0.2541 - val_loss: 0.1948 - val_mae: 0.1948 - lr: 2.2877e-04\n",
      "Epoch 96/100\n",
      "558/558 [==============================] - 138s 248ms/step - loss: 0.2525 - mae: 0.2525 - val_loss: 0.0540 - val_mae: 0.0540 - lr: 2.2877e-04\n",
      "Epoch 97/100\n",
      "558/558 [==============================] - ETA: 0s - loss: 0.2537 - mae: 0.2537\n",
      "Epoch 97: ReduceLROnPlateau reducing learning rate to 0.00020589114428730683.\n",
      "558/558 [==============================] - 138s 248ms/step - loss: 0.2537 - mae: 0.2537 - val_loss: 0.0498 - val_mae: 0.0498 - lr: 2.2877e-04\n",
      "Epoch 98/100\n",
      "558/558 [==============================] - 139s 248ms/step - loss: 0.2548 - mae: 0.2548 - val_loss: 0.0412 - val_mae: 0.0412 - lr: 2.0589e-04\n",
      "Epoch 99/100\n",
      "558/558 [==============================] - 142s 255ms/step - loss: 0.2546 - mae: 0.2546 - val_loss: 0.0534 - val_mae: 0.0534 - lr: 2.0589e-04\n",
      "Epoch 100/100\n",
      "558/558 [==============================] - 135s 242ms/step - loss: 0.2497 - mae: 0.2497 - val_loss: 0.0729 - val_mae: 0.0729 - lr: 2.0589e-04\n"
     ]
    }
   ],
   "source": [
    "path = \"CutomData[ISL]\"\n",
    "files=os.listdir(path)\n",
    "files.sort()\n",
    "\n",
    "image_arr=[]\n",
    "label_arr=[]\n",
    "\n",
    "#loop through each file\n",
    "for i in range(len(files)):\n",
    "    #list of images in each folder\n",
    "    sub_file = os.listdir(path+\"/\"+files[i])\n",
    "    #loop through each sub_file\n",
    "    for j in range(len(sub_file)):\n",
    "        file_path = path+\"/\"+files[i]+\"/\"+sub_file[j]\n",
    "        image = cv2.imread(file_path)\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        image_arr.append(image)\n",
    "        label_arr.append(i)\n",
    "\n",
    "#now converting list to numpy array\n",
    "image_arr = np.array(image_arr)\n",
    "label_arr = np.array(label_arr,dtype=\"float\")\n",
    "#splitting training and testing data\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(image_arr,label_arr,test_size=0.15)\n",
    "\n",
    "del image_arr,label_arr\n",
    "\n",
    "# to free memory \n",
    "gc.collect()\n",
    "\n",
    "model=Sequential()\n",
    "#add pretraind model to sequential model, we are using EfficientNetB0\n",
    "pretrained_model = tf.keras.applications.EfficientNetB0(input_shape=(128,128,3), include_top=False)\n",
    "model.add(pretrained_model)\n",
    "\n",
    "#adding Pooling layer\n",
    "model.add(layers.GlobalAveragePooling2D())\n",
    "\n",
    "#adding Dropout layer\n",
    "#we create dropout to increase accuracy by reducing overfitting\n",
    "model.add(layers.Dropout(0.3))\n",
    "\n",
    "#adding Dense layer as an output\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "#for some tensorflow models we require to built model\n",
    "model.build(input_shape=(None,128,128,3))\n",
    "\n",
    "#compile model\n",
    "model.compile(optimizer=\"adam\",loss=\"mae\",metrics=[\"mae\"])\n",
    "\n",
    "#create check point to save best accuracy model\n",
    "ckp_path = \"trained_model\\model\"\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=ckp_path,monitor=\"val_mae\",mode=\"auto\",save_best_only=True,save_weights_only=True)\n",
    "#monitor: Monitor validation mae loss to save model\n",
    "#mode: Use to save model when val_mae is maximum or minimum, it has 3 options \"min\",\"max\",\"auto\" for us we can select \"min\" or \"auto\"\n",
    "#when val_mae reduce model will be saved\n",
    "#save_best_only=False : It will save all model\n",
    "#save_weights_only=True : Save only weights\n",
    "\n",
    "#creating learning rate reducer to reduce lr when accuracy does not improve\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(factor=0.9,monitor=\"val_mae\",mode=\"auto\",cooldown=0,patience=5,verbose=1,min_lr=1e-6)\n",
    "#factor: when it is reduce next lr will be 0.9 times of current\n",
    "#patience=X\n",
    "#reduce lr after X epoch when accuracy dose not improve\n",
    "#verbose: show it after every epoch\n",
    "#min_lr: minimum learning rate\n",
    "\n",
    "#start training model\n",
    "Epoch=100\n",
    "Batch_size=32\n",
    "#select batch size according to your graphics card\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "train_gen = DataGenerator(X_train, Y_train, 32)\n",
    "test_gen = DataGenerator(X_test, Y_test, 32)\n",
    "#X_train,X_test,Y_train,Y_test\n",
    "history = model.fit(train_gen,\n",
    "                    validation_data=test_gen,\n",
    "                    batch_size=Batch_size,\n",
    "                    epochs=Epoch,\n",
    "                    callbacks=[model_checkpoint,reduce_lr])\n",
    "#Before training you can delete image_arr and lable_arr to increase RAM Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 81). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\UTKARS~1\\AppData\\Local\\Temp\\tmpmjc2gvzp\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\UTKARS~1\\AppData\\Local\\Temp\\tmpmjc2gvzp\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99/99 [==============================] - 8s 54ms/step\n",
      "[[19.98782  ]\n",
      " [29.83055  ]\n",
      " [30.02751  ]\n",
      " [22.021828 ]\n",
      " [22.02156  ]\n",
      " [ 4.947673 ]\n",
      " [ 4.949088 ]\n",
      " [27.859747 ]\n",
      " [ 2.9626722]\n",
      " [23.006702 ]]\n",
      "[20. 30. 30. 22. 22.  5.  5. 28.  3. 23.]\n"
     ]
    }
   ],
   "source": [
    "# after the training is done load best model\n",
    "\n",
    "model.load_weights(ckp_path)\n",
    "\n",
    "# convert model to tensorflow lite model\n",
    "\n",
    "converter=tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model=converter.convert()\n",
    "\n",
    "# save model\n",
    "with open(\"model.tflite\",\"wb\") as f:\n",
    "\tf.write(tflite_model)\n",
    "\n",
    "# if you want to see prediction result on test dataset\n",
    "prediction_val=model.predict(X_test,batch_size=32)\n",
    "\n",
    "# print first 10 values\n",
    "print(prediction_val[:10])\n",
    "# print first 10 values of Y_test\n",
    "print(Y_test[:10])\n",
    "\n",
    "# Save and run this python file\n",
    "# Before that I will show you\n",
    "# loss: 0.4074 - mae: 0.4074 - val_loss: 0.3797 - val_mae: 0.3797\n",
    "# we have mae and val_mae:\n",
    "# mae: Is on X_train\n",
    "# val_mae: X_test\n",
    "# If val_mae is reducing that means your model is improving."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1afac5fc6a59ac64e555c2c5a53be0114a5ddd7f2cf57928dcefec2be136f4d8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
